\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\title{Progress report}
\author{Lukas Zorich}

\maketitle

\section{Background}

Let $X = \{ x_1, \dotsc, x_N \}$ a collection of $N$ points. Now, lets 
suppose that the features of $x_j$ are updated, and lets call $x_j^{\star}$
the updated point. Lastly, lets define 
$X_{-j} = \{ x_1, \dotsc, x_{j-1}, x_{j+1}, \dotsc, x_N \}$.

The posterior before $x_j$ moves, and using $P(\Theta)$ as the prior is

$$P(\Theta \mid X) \propto P(X \mid \Theta)P(\Theta)$$,

where $P(X \mid \Theta) = \prod_i^N P(x_i \mid \Theta)$.

Now, after $x_j$ moves to $x_j^{\star}$, the posterior is

\begin{align}
P(\Theta \mid X_{-j}, x_j^{\star}) &\propto P(X_{-j}, x_j^{\star} \mid \Theta)
    P(\Theta) \\
&\propto P(X_{-j} \mid \Theta)P(x_j^{\star} \mid \Theta)
    P(\Theta) \\
&\propto \frac{P(X \mid \Theta)}{P(x_j \mid \Theta)}P(x_j^{\star} \mid \Theta)
    P(\Theta) \\
&\propto \frac{P(x_j^{\star} \mid \Theta)}{P(x_j \mid \Theta)}P(X \mid \Theta)
    P(\Theta) \\
\end{align}

which can be written as,

\begin{equation}
\label{eq-1}
P(\Theta \mid X_{-j}, x_j^{\star}) \propto 
\frac{P(x_j^{\star} \mid \Theta)}{P(x_j \mid \Theta)}P(\Theta \mid X)
\end{equation}

Now, if we instead of considering one data point $x_j$, we consider a batch
of $S$ data points $X_J = \{ x_j, x_{j+1}, \dotsc, x_{j + S - 1}, x_{j + S} \}$
that moved from $x_{j + k}$ to $x_{j + k}^{\star}$, for $k = 0, \dotsc, S$. And
lets define $X_{-J} = \{ x_1, \dotsc, x_{j-1}, x_{j + S + 1}, \dotsc, x_N \}$.
Then, Eq. \ref{eq-1} for the batch of $S$ data point and because the prior
$P(\Theta)$ gets canceled, can be written as,

\begin{equation}
\label{eq-2}
P(\Theta \mid X_{-J}, X_{J}^{\star}) \propto 
\frac{P(\Theta \mid X_{J}^{\star})}{P(\Theta \mid X_{-J})}P(\Theta \mid X) \\
\end{equation}.

Inspired by the work done in \cite{streaming-variational-bayes}, we assume 
that we approximate the posterior using \textbf{variational inference}. Also,
we assume that $P(\Theta)$ is an exponential family distribution for $\Theta$
with sufficient statistic $T(\Theta)$ and natural parameter $\xi_0$. We suppose
further that if $q(\Theta)$ is the approximate posterior obtained using
variational inference, then $q(\Theta)$ is also in the same exponential family
with a parameter $\xi$ such that

\begin{equation}
\label{exponential-family}
q(\Theta) \propto \text{exp}(\xi \cdot T(\Theta)).
\end{equation}

Similar to \cite{streaming-variational-bayes}, when we make this assumptions
the update in Eq. \ref{eq-2} becomes

\begin{equation}
P(\Theta \mid X_{-J}, X_{J}^{\star}) \approx 
\text{exp}([\xi - \xi_{J} + \xi_{J}^{\star}] \cdot T(\Theta))
\end{equation},

where $\xi$ is the natural parameter of $q(\Theta) \approx P(\Theta \mid X)$,
and $\xi_{J}$ and $\xi_{J}^{\star}$ corresponds to the natural parameter of 
$q_{J}(\Theta) \approx p(\Theta \mid X_{J})$ and 
$q_{J}^{\star}(\Theta) \approx p(\Theta \mid X_{J}^{\star})$ respectively.

\textbf{Using this approach, we can update the posterior when data "moves" 
without the need to go through the whole dataset, instead we just need to go 
through the data points that moved to obtain $\xi_{J}$ and $\xi_{J}^{\star}$.}

\section{Application to my model}
For trying the proposed approach, I'm starting only with a single GMM (I prefer
to start small).

For a GMM, the update would be

\begin{align}
P(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} 
    \mid X_{-J}, X_J^{\star}) 
&\approx \frac{q_{-J}^{\star}(\boldsymbol{\pi}, \boldsymbol{\mu}, 
    \boldsymbol{\Lambda})}
    {q_{-J}(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})}
    q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) \\
\end{align}

where 

\begin{align}
q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) 
&= q(\boldsymbol{\pi})p(\boldsymbol{\mu}, \boldsymbol{\Lambda}) \\
&= Dir(\boldsymbol{\pi} \mid \boldsymbol{\alpha})
\mathcal{N}(\boldsymbol{\mu} \mid \mathbf{m}, (\beta\boldsymbol{\Lambda})^{-1})
\mathcal{W}(\boldsymbol{\Lambda} \mid \mathbf{W}, \nu).
\end{align}

\subsection{Updates}

\subsubsection{Dirichlet}

The natural parameter for the dirichlet is:

\begin{equation}
\xi = \boldsymbol{\alpha} - 1
\end{equation},

hence, the update is:

\begin{equation}
\boldsymbol{\alpha}' = \boldsymbol{\alpha} 
- \boldsymbol{\alpha}_J 
+ \boldsymbol{\alpha}_J^{\star}
\end{equation}

\subsubsection{Normal-Wishart}

The natural parameter for the Normal-Wishart distribution is:

\begin{equation}
\xi = \begin{bmatrix}
       \beta\boldsymbol{\mu}           \\[0.3em]
       \beta \\[0.3em]
       \boldsymbol{\Lambda}^{-1} + \beta\boldsymbol{\mu}\boldsymbol{\mu}^T \\[0.3em]
       \nu + 2 + p
     \end{bmatrix}
\end{equation},

hence, the updates are:

\begin{align}
\boldsymbol{\mu}' &= \frac{1}{\beta '}(\beta\boldsymbol{\mu} 
        - \beta_J\boldsymbol{\mu}_J 
        + \beta_J^{\star}\boldsymbol{\mu}_J^{\star}) \\
\beta' &= \beta - \beta_J + \beta_J^{\star} \\
\boldsymbol{\Lambda}^{-1}' &= 
    (\boldsymbol{\Lambda}^{-1} + \beta\boldsymbol{\mu}\boldsymbol{\mu}^T)
    - (\boldsymbol{\Lambda}_J^{-1} 
        + \beta_J\boldsymbol{\mu}_J\boldsymbol{\mu}_J^T)
    + (\boldsymbol{\Lambda}_J^{-1\star} 
        + \beta_J^{\star}\boldsymbol{\mu}_J^{\star}\boldsymbol{\mu}_J^{T\star})
    - \beta'\boldsymbol{\mu}'\boldsymbol{\mu}^T' \\
\nu' &= \nu - \nu_J + \nu_J^{\star} \\
\end{align}

\section{Experiments}
Write your conclusion here.
Hola \cite{streaming-variational-bayes} blabla

\section{Next steps}

\bibliographystyle{acm}
\bibliography{bibliography} 
\end{document}
